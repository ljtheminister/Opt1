\documentclass{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx, array}

\begin{document}

\title{IEOR 6613 - Optimization I\\ HW 5:  4.43, 5.3, 5.5, 5.12, 5.14, 6.1, 6.4}

\author{John Min\\ jcm2199}
\date{October 30, 2013}
\maketitle

\section*{4.43}

\textbf{(a)}  Consider the minimization of $c_1 x_1 + c_2 x_2$ subject to the constraints:
$x_2 - 3 \leq x_1 \leq 2 x_2 +2, \; x_1, x_2 \geq 0$. \\
\noindent 
Find necessary and sufficient conditions on $(c_1, c_2)$ for the optimal cost to be finite.\\

\noindent
The dual becomes a maximization of $3 p_1 + 2 p_2$ subject to the constraints: $- p_1 + p_2 \geq c_1, p_1 - 2 p_2 \geq c_2, p_1, p_2 \leq 0$.  We get two constraints from the dual for a finite optimal cost where $p_1 = p_2 = 0.$ \\
$c_1 + c_2 \geq 0$ and $2 c_1 + c_2 \geq 0$. \\

\noindent
\textbf{(b)}
For a general feasible linear programming problem, consider the set of all cost vectors for which the optimal cost is finite.  Is it a poyhedron?  Prove your answer. \\

\noindent
Yes.  Any general LP can be rewritten into the form of minimizing $\mathbf{c'x}$ subject to $\mathbf{Ax \geq b}$.  By Theorem 4.14, the LP has an unbounded optimal cost if and only if some extreme ray $\mathbf{d}$ of the feasible set satisfying $\mathbf{c'd} < 0$.  For the optimal cost to be bounded, we need $\mathbf{c'd \geq 0}$ for any $\mathbf{d}$, which represents a polyhedron. \\

\section*{5.3 (Sensitivity with respect to changes in a basic column of A)}
Consider a linear programming problem in standard form under the usual assumption that the rows of the matrix $\mathbf{A}$ are linearly independent.  Suppose that the columns $\mathbf{A_1, \ldots, A_m}$ form an optimal basis.  Let $\mathbf{A_0}$ be some vector and suppose that we change $\mathbf{A_1}$ to $\mathbf{A_1} + \delta \mathbf{A_0}$.  Consider the matrix $\mathbf{B}(\delta)$ consisting of the columns $\mathbf{A_1} + \delta \mathbf{A_0, A_2, \ldots, A_m}$.  Let $[\delta_1, \delta_2]$ be a closed interval of values of $\delta$ that contains zero and in which the determinant of $\mathbf{B}(\delta)$ is nonzero.  Show that the subset of $[\delta_1, \delta_2]$ for which $\mathbf{B}(\delta)$ is an optimal basis is also a closed interval.  \\


\noindent
The Sherman-Morrison formula states the following:  $\mathbf{B(\delta)^{-1} = B^{-1} - } \frac{ \mathbf{B^{-1} A_0 e_1} \delta }{1 + \delta \mathbf{e_1' B^{-1} A_0}} $. \\
For $\mathbf{B}(\delta)$ to be an optimal basis, we need to check for two conditions:
\begin{itemize}
	\item $\mathbf{B}(\delta)^{-1} \mathbf{b} \geq 0$ (feasibility) 
	\item $\mathbf{ \bar{c} = c' - c_B' B(\delta)^{-1}A \geq 0 }$ (optimality)
\end{itemize}

\noindent
Clearly, $0 \in [\delta_1, \delta_2].$  If there is no nonzero $\delta$ such that both the feasibility and optimality conditions hold, we still have a closed interval.  Let $\delta^1, \ldots, \delta^N$ be a sequence that converges to $\delta$.  There exists $\delta$ for which the feasibility constraint is active and perhaps the same or another $\delta$ for which the optimality is constraint is active.  The set of possible $\delta$ in maintaining feasibility contains all of its limit points and thus is closed.  This is also true for the set of $\delta$ to maintain optimality.  The intersection of two closed sets is closed.  Therefore, this optimal basis holds for a closed interval of $\delta$.

\section*{5.5}

\textbf{(a)} Give necessary and sufficient conditions for the basis descibed by this tableau to be optimal. \\

\noindent 
From the tableau, we see that $\mathbf{B^{-1} [A_3 A_4 A_5] = B^{-1} I = B^{-1} = } 
\begin{bmatrix}
-1 & 0 & \beta \\
 2 & 1 & \gamma \\
 4 & 0 & \delta
\end{bmatrix}
$ \\
Certainly, det($\mathbf{B^{-1}}) \neq 0 \Leftrightarrow \delta \neq 4 \beta$. \\

\noindent If $\bar{c}_3, \bar{c}_5 \geq 0$, then, the basis is optimal by Definition 3.3 since all the basic variables are nonnegative ($\mathbf{B^{-1}b \geq 0}$). For these nonnegative reduced costs, we need $c_3 - \mathbf{c_B'} [-1 \; 2 \; 4] \geq 0 \text{ and } c_5 - \mathbf{c_B'} [\beta \;  \gamma \; \delta] \geq 0$.\\

\noindent
\textbf{(b)} Assume that this basis is optimal and that $\bar{c}_3 = 0$.  Find an optimal basic feasible solution, other than the one described by this tableau. \\

\noindent
To find another optimal BFS, $x_3$ will enter the basis since $\bar{c}_3 = 0$.  $x_1$ will exit the basis from the minimum ratio test and we get the following tableau:
\noindent $ x_2 = 7/4, x_4 = 1/2, x_3 = 3/4 $. \\

\begin{tabular}{ |>{$}c<{$} | >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$}| }
\hline
& x_1 & x_2 & x_3 & x_4 & x_5 \\
\hline
& 0 & 0 & 0 & 0 & \bar{c}_5 \\
\hline
x_2 = \frac{7}{4} & 0 & 1 & 0 & 0 & \beta + \frac{\delta}{4} \\
x_4 = \frac{1}{2} & 0 & 0 & 0 & 1 & \gamma - \frac{\delta}{2} \\
x_3 = \frac{3}{4} & \frac{1}{4} & 0 & 1 & 0 & \frac{\delta}{4} \\
\hline
\end{tabular} \\


\noindent
\textbf{(c)} Suppose that $\gamma > 0$.  Show that there exists an optimal basic feasible solution, regardless of the values of $\bar{c}_3$ and $\bar{c}_5$.   \\

\noindent
Of course, if the reduced costs are nonnegative, we have reached an optimal BFS at this iteration.  If $\bar{c}_3 < 0$, $x_3$ can enter the basis with either $x_1$ leaving the basis and reduce the cost and $\bar{c}_3$ becomes 0.  We are done and have reached an optimal basis if $\bar{c}_5 \geq 0$.  Otherwise, we are left with only one negative reduced cost, $\bar{c}_5$.   With $c_5 < 0$ and $\gamma > 0$, $x_5$ can then enter the basis for $x_4$ and we have nonnegative reduced costs and an optimal BFS.  We will not reach a point in a simplex iteration where no compunent of $\mathbf{u}$ is positive such that the simplex terminates and we conclude that the optimal cost is $-\infty$.\\

\noindent
\textbf{(d)}  Assume that the basis associated with this tableau is optimal.  Suppose also that $b_1$ in the original problem is replaced by $b_1 + \epsilon$.  Give upper and lower bounds on $\epsilon$ so that this basis remains optimal. \\
 
\noindent 
Optimality conditions are not affected by the changes in $\mathbf{b}$.  Therefore, we only need to check for feasibility:  $\mathbf{B^{-1}(b } + \delta \mathbf{e_1) \geq 0}$.  Let $\mathbf{g}$ be the first column of $\mathbf{B^{-1}}$.  \\
We have $\mathbf{x_B} + \delta \mathbf{g} \geq 0 \Rightarrow 
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}
+ \epsilon
\begin{bmatrix}
-1 \\
2 \\
4
\end{bmatrix}
\geq 0
\Rightarrow - \frac{3}{4} \leq \epsilon \leq 1 $. \\

\noindent
\textbf{(e)}  Assume that the basis associated with this tableau is optimal.  Suppose also that $c_1$ in the original problem is replaced by $c_1 + \epsilon$.  Give upper and lower bounds on $\epsilon$ so that this basis remains optimal. \\

\noindent
Primal feasibility is not affected.  The new optimality condition is:  $(\mathbf{c_B} + \epsilon \mathbf{e_1)' B^{-1} A_i \leq } c_i, \; \forall i \neq j = 1$. $x_2, x_4$ are basic and their reduced cost will remain zero, thus, we look at $x_3$ and $x_5.$ We get the following conditions: \\
\begin{equation*}
\begin{aligned}
\epsilon \leq \bar{c}_3 / {4} \\
\epsilon \leq \bar{c}_5 / \delta
\end{aligned}
\end{equation*}

\section*{5.12}
\textbf{(a)}  Suppose that for some value of $\theta$, there are exactly two distinct basic feasible solutions that are optimal.  Show that they must be adjacent.  \\

\noindent
Let $ \mathbf{x^*, y^*}$ be the two distinct optimal BFS for the parametric programming problem.  Since they are optimal and for some fixed $\theta$, $(\mathbf{c} + \theta \mathbf{d})\mathbf{' x^*} = (\mathbf{c} + \theta \mathbf{d})\mathbf{' y^*}$.  If they are adjacent optimal BFS, then, given some iteration in the simplex where optimality has been achieved, one can take a nonbasic variable $x_j$ with $\bar{c}_j = 0$ to traverse the basic feasible direction between the two optimal solutions.  \\

\noindent
Suppose they are not adjacent.  It means that we cannot replace a basic variable with a nonbasic one with zero reduced cost to traverse between $x^*$ and $y^*$.  If the basic feasible direction to traverse between the two basic feasible solutions is associated with a nonzero cost, clearly, the two BFS cannot be both optimal because they have different costs.  Otherwise,  if there exists a BFS, $z^*$, between $x^*$ and $y^*$, which we can traverse thru to get from the two BFS with zero change in the cost which leads to there being more than two distinct optimal BFS.  Therefore, they must be adjacent. \\

\noindent
\textbf{(b)}
Consider the following parametric programming problem: \\
\begin{equation*}
\begin{aligned}
& \text{minimize} && -x_1 - x_2 - x_3 + \theta x_4 - \theta x_5 \\
& \text{subject to} &&  x_1 + x_2 + x_3 = 1\\
& & & x_1 + x_4 = 1 \\
& & & x_2 + x_5 = 1 \\
& & &\mathbf{x \geq 0}
\end{aligned}
\end{equation*}

\noindent
Let $\theta^* = 0$.  It is clear that (0, 1, 0, 1, 0) can be our $\mathbf{x^1}$, our unique solution for $\theta < 0$.   (1, 0, 0, 0, 1) can be $\mathbf{x^3}$, the unique optimal BFS for $\theta > 0$.   And our $\mathbf{x^2}$ is (0, 0, 1, 1, 1) for $\theta = 0$.  Clearly, since none of these BFS are adjacent. \\

\section*{5.14}
\textbf{(a)} Suppose that a certain basis is optimal for $\theta = -10$ and for $\theta = 10$.  Prove that the same basis is optimal for $\theta = 5$.  \\

\noindent
$\mathbf{(c - 10d)' x = (c + 10d)' x \Rightarrow -10d'x = 10d'x \Rightarrow 20d'x = 0 \Rightarrow d'x = 0}$.  The same idea holds for $\mathbf{Ax = b + \theta f}$.  Therefore, $5 \mathbf{d'x} = 0$.  As the optimality and feasibility conditions hold, the same basis remains optimal. \\

\noindent 
Perhaps the more elegant solution would have been to multiply the objective function and constraints for $\theta = -10$ by $\frac{1}{4}$ and those for $\theta = 10$ by $\frac{3}{4}$.  By summing the linear combinations of the objective function and constraint equations, it is clear that we maintain optimality and feasibility for $\theta = 5$.  \\

\noindent
\textbf{(b)}  Show that $f(\theta)$ is a piecewise quadratic function of $\theta$.  Give an upper bound on the number of "pieces". \\

\noindent
Let $\mathbf{B_j^{-1}}$ be an arbitrary basis.  Let us substitute the constraints into the objective function. \\
$f(\theta) = \underset{j}{\text{min}} \{ \mathbf{(c+ \theta d)' B_j^{-1}(b + \theta f)}\}$ where $\mathbf{B_j^{-1} (b + \theta f) \geq 0}$. \\
$f(\theta) = \underset{j}{\text{min}} \{ \mathbf{ d' B_j^{-1} f \theta^2 + (c'B_j^{-1} f + d'B_j^{-1} b)\theta  + c' B_j^{-1} b }\} $ where $\mathbf{B_j^{-1} (b + \theta f) \geq 0}$. \\
Clearly, $f(\theta)$ is a piecewise quadratic function of $\theta$.  \\

\noindent
Let $K$ be the number of possible bases (the cardinality $|j|$.  The upper bound on the number of pieces is $2K$.\\

\noindent
\textbf{(c)}  Let $\mathbf{b = 0 \text{ and } c = 0}$.  Suppose that a certain basis is optimal for $\theta = 1$.  For what other nonnegative values of $\theta$ is that same basis optimal? \\

\noindent
\begin{equation*}
\begin{aligned}
& \text{minimize} && \theta\mathbf{d'x} \\
& \text{subject to} && \mathbf{Ax = } \theta \mathbf{x} \\
& && \mathbf{x \geq 0}
\end{aligned}
\end{equation*}

\noindent
Let $\mathbf{B}$ be an optimal basis for $\theta = 1$ and assume that $\theta > 0$.  $\mathbf{d' - d'_B B^{-1} A \geq 0 \text{ and } B^{-1}f \geq 0}$.  Hence for nonnegative $\theta$ satisfying $\theta \mathbf{d' - d'_B B^{-1} A \geq 0 \text{ and } } \theta \mathbf{B^{-1}f \geq 0}$ keeps this same basis optimal. \\

\noindent
\textbf{(d)} Is $f(\theta)$ convex, concave, or neither? \\

\noindent
$f(\theta)$ can be neither.  Consider $\mathbf{b, f = 0}$.

\section*{6.1}
Consider the cutting stock probelm.  Use an optimal solution to the linear programming problem (6.4) to construct a feasible solution for the corresponding integer programming problem, whose cost differs from the optimal cost by no more than $m$.  \\

\noindent
Apply the ceiling cunction to our decision variables, which will round up the non-integer decision variables to the next integer.  At most $m$ non-zero variables can be rounded up and thus, the IP-optimal cost $\leq$ LP-optimal cost + $m$.

\section*{6.4}

\noindent
\textbf{(a)}
Form the dual problem and explain how Dantzig-wolfe decomposition can be applied to it.  What is the structure of the subproblems solved during a typical iteration? \\

\noindent
\begin{equation*}
\begin{aligned}
& \text{maximize} && \mathbf{p_1' b_1 + p_2' b_2} \\
& \text{subject to} &&  \mathbf{F_1 p_1 + F_2 p_2 = c_o} \\
& & & \mathbf{D_1 p_1 \leq c_1} \\
& & & \mathbf{D_2 p_2 \leq c_2} \\
& & & \mathbf{p_1, p_2 \geq 0}
\end{aligned}
\end{equation*}

\noindent
We have two subproblems that are generalized out of this form: $\mathbf{b_1' - q'F_1 p_1}$ being minimized subject to $\mathbf{p_1} \in P_1$ meaning that $\mathbf{D_1 p_1 \leq c_1}$.  Our second subproblem is analogous to the first, except using $i = 2$. \\

\noindent
\textbf{(b)} Rewrite the first set of constraints in the form $\mathbf{D_1 x_1 + F_1 y_1 \geq b_1}$ and $\mathbf{D_2 x_2 + F_2 y_2 \geq b_2}$, together with a constraint relating $\mathbf{y_1 \text{to } y_2}$.  Discuss how to apply Dantzig-Wolfe decomposition and describe the structure of the subproblems solved during a typical iteration. \\

\noindent
\begin{equation*}
\begin{aligned}
& \text{minimize} && \mathbf{c_1' x_1 + c_2' x_2 + c_0' y_1 + c_0' y_2} \\
& \text{subject to} && \mathbf{\begin{bmatrix} \mathbf{0} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{e}
\end{bmatrix} z_1 - \begin{bmatrix} \mathbf{0} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{e}
\end{bmatrix} z_2 = 0} \\
&&&  \begin{bmatrix} \mathbf{D_1} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{F_1}
\end{bmatrix} \mathbf{z_1 \geq b_1} \\
&&&  \begin{bmatrix} \mathbf{D_2} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{F_2}
\end{bmatrix} \mathbf{z_2 \geq b_2} \\
\end{aligned}
\end{equation*}

\noindent
Having defined $\mathbf{z_i = [x_i \; y_i]}$ for $i = 1,2$ and $\mathbf{M = } \begin{bmatrix} \mathbf{0} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{e}
\end{bmatrix}$ , we have reformulated the primal into Dantiz-Wolfe decomposition.  The subproblems look as follows:  minimize $\mathbf{(c_1' + c_0' - q'M) z_i}$ subject to $\mathbf{z_i} \in P_i$ for $i = 1,2$.    



\end{document}
