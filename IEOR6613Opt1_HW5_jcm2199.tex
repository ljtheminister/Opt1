\documentclass{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx}

\begin{document}

\title{IEOR 6613 - Optimization I\\ HW 5:  4.43, 5.3, 5.5, 5.12, 5.14, 6.1, 6.4}

\author{John Min\\ jcm2199}
\date{October 30, 2013}
\maketitle

\section*{4.43}

\textbf{(a)}  Consider the minimization of $c_1 x_1 + c_2 x_2$ subject to the constraints:
$x_2 - 3 \leq x_1 \leq 2 x_2 +2, \; x_1, x_2 \geq 0$. \\
\noindent 
Find necessary and sufficient conditions on $(c_1, c_2)$ for the optimal cost to be finite.\\

\noindent
The dual becomes a maximization of $3 p_1 + 2 p_2$ subject to the constraints: $- p_1 + p_2 \geq c_1, p_1 - p_2 \geq c_2, p_1, p_2 \leq 0$.  If we sum the first two constraint equations, we see that $c_1 + c_2 \leq 0$ is a constraint.  Since the feasible set of the primal is unbounded, only $c_1 \geq 0, c_2 \geq 0$ will force the optimal cost to be finite.  If a $c_j > 0$, the optimal $x^*_j = 0$.  If one or both of the $c_j$'s are negative, the primal becomes unbounded as we can drive the $x_j$ towards $\infty$. \\


\noindent
\textbf{(b)}
For a general feasible linear programming problem, consider the set of all cost vectors for which the optimal cost is finite.  Is it a poyhedron?  Prove your answer. \\

\noindent
Yes.  Any general LP can be rewritten into the form of minimizing $\mathbf{c'x}$ subject to $\mathbf{Ax \geq b}$.  By Theorem 4.14, the LP has an unbounded optimal cost if and only if some extreme ray $\mathbf{d}$ of the feasible set satisfying $\mathbf{c'd} < 0$.  For the optimal cost to be bounded, we need $\mathbf{c'd \geq 0}$ for any $\mathbf{d}$, which represents a polyhedron. \\

\section*{5.3 (Sensitivity with respect to changes in a basic column of A)}
Consider a linear programming problem in standard form under the usual assumption that the rows of the matrix $\mathbf{A}$ are linearly independent.  Suppose that the columns $\mathbf{A_1, \ldots, A_m}$ form an optimal basis.  Let $\mathbf{A_0}$ be some vector and suppose that we change $\mathbf{A_1}$ to $\mathbf{A_1} + \delta \mathbf{A_0}$.  Consider the matrix $\mathbf{B}(\delta)$ consisting of the columns $\mathbf{A_1} + \delta \mathbf{A_0, A_2, \ldots, A_m}$.  Let $[\delta_1, \delta_2]$ be a closed interval of values of $\delta$ that contains zero and in which the determinant of $\mathbf{B}(\delta)$ is nonzero.  Show that the subset of $[\delta_1, \delta_2]$ for which $\mathbf{B}(\delta)$ is an optimal basis is also a closed interval.  \\


\noindent
The Sherman-Morrison formula states the following:  $\mathbf{B(\delta)^{-1} = B^{-1} - } \frac{ \mathbf{B^{-1} A_0 e_1} \delta }{1 + \delta \mathbf{e_1' B^{-1} A_0}} $. \\
For $\mathbf{B}(\delta)$ to be an optimal basis, we need to check for two conditions:
\begin{itemize}
	\item $\mathbf{B}(\delta)^{-1} \mathbf{b} \geq 0$ (feasibility) 
	\item $\mathbf{ \bar{c} = c' - c_B' B(\delta)^{-1}A \geq 0 }$ (optimality)
\end{itemize}

\noindent
Clearly, $0 \in [\delta_1, \delta_2].$  If there is no nonzero $\delta$ such that both the feasibility and optimality conditions hold, we still have a closed interval.  Otherwise, we have $\delta$ amount of change with respect to the second term in the Sherman-Morrison formula, driving the basic variables and/or reduced costs to 0.  Some $\delta_1$ and $\delta_2$ will eventually drive a basic variable and reduced cost to 0, respectively, that was originally positive.  Since these $\delta$ values can be included in this range of optimal $\delta$, we have a closed interval, one that also includes 0. \\



\section*{5.5}

\textbf{(a)} Give necessary and sufficient conditions for the basis descibed by this tableau to be optimal. \\

\noindent 
From the tableau, we see that $\mathbf{B^{-1} [A_3 A_4 A_5] = B^{-1} I = B^{-1} = } 
\begin{bmatrix}
-1 & 0 & \beta \\
 2 & 1 & \gamma \\
 4 & 0 & \delta
\end{bmatrix}
$ \\
Certainly, det($\mathbf{B^{-1}}) \neq 0 \Leftrightarrow \delta \neq 4 \beta$. \\

\noindent If $\bar{c}_3, \bar{c}_5 \geq 0$, then, the basis is optimal by Definition 3.3 since all the basic variables are nonnegative ($\mathbf{B^{-1}b \geq 0}$). For these nonnegative reduced costs, we need $c_3 - \mathbf{c_B'} [-1 \; 2 \; 4] \geq 0 \text{ and } c_5 - \mathbf{c_B'} [\beta \;  \gamma \; \delta] \geq 0$.\\

\noindent
\textbf{(b)} Assume that this basis is optimal and that $\bar{c}_3 = 0$.  Find an optimal basic feasible solution, other than the one described by this tableau. \\

\noindent $ x_1 = 3, x_2 = 1, x_3 = 1 $. \\

\textbf{(c)} Suppose that $\gamma > 0$.  Show that there exists an optimal basic feasible solution, regardless of the values of $\bar{c}_3$ and $\bar{c}_5$.   \\

\noindent
Of course, if the reduced costs are nonnegative, we have reached an optimal BFS at this iteration.  If $c_3 < 0$, $x_3$ can enter the basis with either $x_1$ or $x_4$ leaving the basis and reduce the cost.  If $c_5 < 0$ and $\gamma > 0$, $x_5$ can enter the basis for $x_4$.  \\

\noindent
\textbf{(d)}  Assume that the basis associated with this tableau is optimal.  Suppose also that $b_1$ in the original problem is replaced by $b_1 + \epsilon$.  Give upper and lower bounds on $\epsilon$ so that this basis remains optimal. \\
 
\noindent 
Optimality conditions are not affected by the changes in $\mathbf{b}$.  Therefore, we only need to check for feasibility:  $\mathbf{B^{-1}(b } + \delta \mathbf{e_1) \geq 0}$.  \\


\textbf{(e)}  Assume that the basis associated with this tableau is optimal.  Suppose also that $c_1$ in the original problem is replaced by $c_1 + \epsilon$.  Give upper and lower bounds on $\epsilon$ so that this basis remains optimal. \\

\noindent $ - \bar{c}_3 / 4 \leq \epsilon \leq \bar{c}_5 / \delta$.

\section*{5.12}
\textbf{(a)}  Suppose that for some value of $\theta$, there are exactly two distinct basic feasible solutions that are optimal.  Show that they must be adjacent.  \\

\noindent
Let $ \mathbf{x^*, y^*}$ be the two distinct optimal BFS for the parametric programming problem.  Since they are optimal and for some fixed $\theta$, $(\mathbf{c} + \theta \mathbf{d})\mathbf{' x^*} = (\mathbf{c} + \theta \mathbf{d})\mathbf{' y^*}$.  If they are adjacent optimal BFS, then, given some iteration in the simplex where optimality has been achieved, one can take a nonbasic variable $x_j$ with $\bar{c}_j = 0$ to traverse the basic feasible direction between the two optimal solutions.  \\

\noindent
Suppose they are not adjacent.  It means that we cannot replace a basic variable with a nonbasic one with zero reduced cost to traverse between $x^*$ and $y^*$.  If the basic feasible direction to traverse between the two basic feasible solutions is associated with a nonzero cost, clearly, the two BFS cannot be both optimal because they have different costs.  Otherwise,  if there exists a BFS, $z^*$, between $x^*$ and $y^*$, which we can traverse thru to get from the two BFS with zero change in the cost which leads to there being more than two distinct optimal BFS.  Therefore, they must be adjacent.

\textbf{(b)}

\section*{5.14}
\textbf{(a)} Suppose that a certain basis is optimal for $\theta = -10$ and for $\theta = 10$.  Prove that the same basis is optimal for $\theta = 5$.  \\

\noindent
$\mathbf{(c - 10d)' x = (c + 10d)' x \Rightarrow -10d'x = 10d'x \Rightarrow 20d'x = 0 \Rightarrow d'x = 0}$.  Therefore, $5 \mathbf{d'x} = 0$ and hence, 5 is an optimal theta for the same basis.  Now, we just need to check feasibility and since $5 \in [-10, 10]$, we know that $\mathbf{Ax = b} + \theta \mathbf{f}$. \\

\noindent
\textbf{(b)}  Show that $f(\theta)$ is a piecewise quadratic function of $\theta$.  Give an upper bound on the number of "pieces". \\

\noindent
If $\mathbf{x} \in \mathbb{R}^n$, then, the number of "pieces" $\leq n$. \\


\noindent
\textbf{(c)}  Let $\mathbf{b = 0 \text{ and } c = 0}$.  Suppose that a certain basis is optimal for $\theta = 1$.  For what other nonnegative values of $\theta$ is that same basis optimal? \\

\noindent



\noindent
\textbf{(d)} Is $f(\theta)$ convex, concave, or neither? \\

\noindent
$f(\theta)$ is a concave 



\end{document}
